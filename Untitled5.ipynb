{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394acb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39289c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44a596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.is_train = True\n",
    "        self.image_size = 33\n",
    "        self.label_size = 33\n",
    "        self.scale = 2\n",
    "        self.stride = 14\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epoch = 10\n",
    "        self.checkpoint_dir = './checkpoint'\n",
    "        self.sample_dir = './sample'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def imread(path, is_grayscale=True):\n",
    "    if is_grayscale:\n",
    "        return np.array(Image.open(path).convert('YCbCr').split()[0], dtype=np.float32)\n",
    "    else:\n",
    "        return np.array(Image.open(path).convert('YCbCr'), dtype=np.float32)\n",
    "\n",
    "def modcrop(image, scale=3):\n",
    "    h, w = image.shape\n",
    "    h = h - np.mod(h, scale)\n",
    "    w = w - np.mod(w, scale)\n",
    "    return image[0:h, 0:w]\n",
    "\n",
    "def preprocess(image_path, scale=3):\n",
    "    image = imread(image_path, is_grayscale=True)\n",
    "    label_ = modcrop(image, scale)\n",
    "    input_ = label_ / 255.0\n",
    "    label_ = label_ / 255.0\n",
    "    return input_, label_\n",
    "\n",
    "def input_setup(config, image_path):\n",
    "    input_, label_ = preprocess(image_path, config.scale)\n",
    "    sub_input_sequence = []\n",
    "    sub_label_sequence = []\n",
    "\n",
    "    padding = abs(config.image_size - config.label_size) // 2\n",
    "\n",
    "    h, w = input_.shape\n",
    "\n",
    "    for x in range(0, h - config.image_size + 1, config.stride):\n",
    "        for y in range(0, w - config.image_size + 1, config.stride):\n",
    "            sub_input = input_[x:x + config.image_size, y:y + config.image_size]\n",
    "            sub_label = label_[x + padding:x + padding + config.label_size, y + padding:y + padding + config.label_size]\n",
    "\n",
    "            sub_input = sub_input.reshape([config.image_size, config.image_size, 1])\n",
    "            sub_label = sub_label.reshape([config.label_size, config.label_size, 1])\n",
    "\n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "    sub_input_sequence = np.array(sub_input_sequence)\n",
    "    sub_label_sequence = np.array(sub_label_sequence)\n",
    "\n",
    "    return sub_input_sequence, sub_label_sequence\n",
    "\n",
    "def imsave(image, path):\n",
    "    Image.fromarray(image.astype(np.uint8)).save(path)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 1))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "# PyTorch Dataset class for handling image data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, config):\n",
    "        self.image_paths = image_paths\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        sub_input_sequence, sub_label_sequence = input_setup(self.config, image_path)\n",
    "        return torch.tensor(sub_input_sequence, dtype=torch.float32).permute(0, 3, 1, 2), \\\n",
    "               torch.tensor(sub_label_sequence, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "# Load images from a directory\n",
    "def prepare_data(dataset):\n",
    "    filenames = [os.path.join(dataset, file) for file in os.listdir(dataset) if file.endswith('.png')]\n",
    "    return filenames\n",
    "\n",
    "# Main code to run the setup\n",
    "dataset_path = \"./images\"  # Directory containing images\n",
    "image_paths = prepare_data(dataset_path)\n",
    "dataset = ImageDataset(image_paths, config)\n",
    "data_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# To visualize the first sub-image and its label\n",
    "sample_input, sample_label = dataset[0]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_input[0][0].numpy(), cmap='gray')\n",
    "plt.title('Sub-input')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_label[0][0].numpy(), cmap='gray')\n",
    "plt.title('Sub-label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a399d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './images/all_images/BSDS200'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8759/3561237355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Load your data as numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./images/all_images/BSDS200'\u001b[0m  \u001b[0;31m# Path to the image file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m train_dataset = data.TensorDataset(torch.tensor(train_data, dtype=torch.float32), \n",
      "\u001b[0;32m/tmp/ipykernel_8759/3561237355.py\u001b[0m in \u001b[0;36minput_setup\u001b[0;34m(config, image_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Load the image and convert it to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './images/all_images/BSDS200'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from math import ceil\n",
    "from skimage.io import imsave\n",
    "from PIL import Image\n",
    "\n",
    "# Helper functions\n",
    "def input_setup(config, image_path):\n",
    "    # Load the image and convert it to a numpy array\n",
    "    image = Image.open(image_path).convert('YCbCr').split()[0]\n",
    "    image = np.array(image, dtype=np.float32)\n",
    "    \n",
    "    sub_input_sequence = []\n",
    "    sub_label_sequence = []\n",
    "\n",
    "    padding = abs(config.image_size - config.label_size) // 2\n",
    "    padded_image = np.pad(image, ((padding, padding), (padding, padding)), 'constant')\n",
    "\n",
    "    for x in range(0, padded_image.shape[0] - config.image_size + 1, config.stride):\n",
    "        for y in range(0, padded_image.shape[1] - config.image_size + 1, config.stride):\n",
    "            sub_input = padded_image[x:x + config.image_size, y:y + config.image_size]\n",
    "            sub_label = padded_image[x + padding:x + padding + config.label_size,\n",
    "                                     y + padding:y + padding + config.label_size]\n",
    "            sub_input_sequence.append(sub_input)\n",
    "            sub_label_sequence.append(sub_label)\n",
    "\n",
    "    # Expand dims to match the required input shape for Conv2D (batch_size, channels, height, width)\n",
    "    sub_input_sequence = np.expand_dims(sub_input_sequence, axis=1)\n",
    "    sub_label_sequence = np.expand_dims(sub_label_sequence, axis=1)\n",
    "    \n",
    "    return np.asarray(sub_input_sequence), np.asarray(sub_label_sequence)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[2], images.shape[3]\n",
    "    img = np.zeros((h * size[0], w * size[1], images.shape[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img.squeeze()\n",
    "\n",
    "class RECONNET(nn.Module):\n",
    "    def __init__(self, image_size=33, label_size=33, measurement_rate=1e-1, c_dim=1):\n",
    "        super(RECONNET, self).__init__()\n",
    "        self.fc_size = int(ceil(measurement_rate * 1089))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1089, self.fc_size)\n",
    "        self.fc2 = nn.Linear(self.fc_size, 1089)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, padding=5)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=7, padding=3)\n",
    "        self.conv4 = nn.Conv2d(1, 64, kernel_size=11, padding=5)\n",
    "        self.conv5 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.conv6 = nn.Conv2d(32, 1, kernel_size=7, padding=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1089)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = x.view(-1, 1, 33, 33)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        return x\n",
    "\n",
    "def train(config, model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(config.epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch: [{epoch+1}], step: [{batch_idx}], time: [{time.time() - start_time:.4f}], loss: [{loss.item():.8f}]\")\n",
    "            if batch_idx % 500 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(config.checkpoint_dir, f'model_epoch_{epoch+1}_step_{batch_idx}.pth'))\n",
    "\n",
    "def test(config, model, device, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            result = output.cpu().numpy()\n",
    "            nx = ny = int(np.sqrt(len(result)))\n",
    "            result = merge(result, [nx, ny])\n",
    "            image_path = os.path.join(config.sample_dir, \"test.png\")\n",
    "            imsave(image_path, result)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.is_train = True\n",
    "        self.image_size = 33\n",
    "        self.label_size = 33\n",
    "        self.scale = 2\n",
    "        self.stride = 14\n",
    "        self.epoch = 10\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-4\n",
    "        self.checkpoint_dir = './checkpoint'\n",
    "        self.sample_dir = './sample'\n",
    "\n",
    "# Main code to run the setup\n",
    "config = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your data as numpy arrays\n",
    "image_path = './images/all_images/BSDS200'  # Path to the image file\n",
    "train_data, train_label = input_setup(config, image_path)\n",
    "\n",
    "train_dataset = data.TensorDataset(torch.tensor(train_data, dtype=torch.float32), \n",
    "                                   torch.tensor(train_label, dtype=torch.float32))\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "model = RECONNET(image_size=config.image_size, label_size=config.label_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if config.is_train:\n",
    "    train(config, model, device, train_loader, optimizer, criterion)\n",
    "else:\n",
    "    test(config, model, device, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b29aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b281d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22538154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
