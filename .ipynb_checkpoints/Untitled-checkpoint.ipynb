{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e217782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n",
      "Images shape on batch size = torch.Size([128, 272])\n",
      "Labels shape on batch size = torch.Size([128, 1089])\n",
      "Epoch: 1 Training Loss: 0.011953177298415952\n",
      "Epoch: 2 Training Loss: 0.00605557985984309\n",
      "Epoch: 3 Training Loss: 0.005607608776624054\n",
      "Epoch: 4 Training Loss: 0.00452811066266705\n",
      "Epoch: 5 Training Loss: 0.004485788169145603\n",
      "Epoch: 6 Training Loss: 0.004185183852453773\n",
      "Epoch: 7 Training Loss: 0.004042448100710992\n",
      "Epoch: 8 Training Loss: 0.003965882134760826\n",
      "Epoch: 9 Training Loss: 0.003749730110276273\n",
      "Epoch: 10 Training Loss: 0.003722924530507988\n",
      "Epoch: 11 Training Loss: 0.0036304199766012783\n",
      "Epoch: 12 Training Loss: 0.0035284348310548417\n",
      "Epoch: 13 Training Loss: 0.003575636051091175\n",
      "Epoch: 14 Training Loss: 0.0034265849260268913\n",
      "Epoch: 15 Training Loss: 0.0034175793591298556\n",
      "Epoch: 16 Training Loss: 0.00334432758759337\n",
      "Epoch: 17 Training Loss: 0.0033635185940870723\n",
      "Epoch: 18 Training Loss: 0.0032900903441348307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30089/441273320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/model_state_new.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30089/441273320.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training Loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from TrainDataset import TrainDataset\n",
    "from TestDataset import TestDataset\n",
    "#from model import ReconNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pyplot\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "\n",
    "# This is the path of the BSDS200 dataset\n",
    "# The Berkeley Segmentation Dataset and Benchmark (BSDS 500 or BSDS 200) is a dataset\n",
    "# and evaluation benchmark widely used in the computer vision community for the task of image segmentation.\n",
    "path = './images/all_images/BSDS200'\n",
    "\n",
    "\n",
    "# We will use the phi_0_25_1089.mat file, the file has a compression ratio of 0.25 and has a dimension of 1089.\n",
    "# We will be using this because it has a higher compression ratio.\n",
    "mat = scipy.io.loadmat('./phi/phi_0_25_1089.mat')['phi']\n",
    "\n",
    "mat = torch.from_numpy(mat)\n",
    "\n",
    "#The train dataset images are compressed and divided into blocks and converted into pytorch tensor.\n",
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = TrainDataset(path,mat,transformations,0.25)\n",
    "\n",
    "train_dl = DataLoader(train_data,batch_size=128)\n",
    "\n",
    "train_iter = iter(train_dl)\n",
    "print(type(train_iter))\n",
    "\n",
    "#images are the compressed images, and labels are the original images, compression ratio - 0.25 is used.\n",
    "#images , labels = train_iter.next()\n",
    "images , labels = next(train_iter)\n",
    "print('Images shape on batch size = {}'.format(images.size()))\n",
    "print('Labels shape on batch size = {}'.format(labels.size()))\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class ReconNet(nn.Module):\n",
    "    def __init__(self, measurement_rate=0.25):\n",
    "        super(ReconNet, self).__init__()\n",
    "        self.measurement_rate = measurement_rate\n",
    "        self.fc1 = nn.Linear(int(self.measurement_rate * 1089), 1089)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 1, 3, 1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 33, 33)  # Reshape to (batch_size, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        return x\n",
    "\n",
    "model = ReconNet(measurement_rate=0.25)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model,criterion,optimizer,train_dl,num_epochs=10):\n",
    "  for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    for inp, lbl in train_dl:\n",
    "      inp = inp.cuda()\n",
    "      lbl = lbl.cuda()\n",
    "      inp = inp.float()\n",
    "      lbl = lbl.float()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      out = model(inp)\n",
    "      out = out.cuda()\n",
    "      out = out.view(lbl.size())\n",
    "      loss = criterion(out,lbl)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    print('Epoch:',epoch+1, 'Training Loss:',np.mean(train_loss))\n",
    "\n",
    "\n",
    "train(model,criterion,optimizer,train_dl,10)\n",
    "\n",
    "torch.save(model.state_dict(), '/model_state_new.pth')\n",
    "\n",
    "state_dict = torch.load('model_state_new.pth',map_location ='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedReconNetWithLSTM(nn.Module):\n",
    "    def __init__(self, measurement_rate=0.25):\n",
    "        super(ModifiedReconNetWithLSTM, self).__init__()\n",
    "        self.measurement_rate = measurement_rate\n",
    "        self.fc1 = nn.Linear(int(self.measurement_rate * 1089), 1089)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.conv8 = nn.Conv2d(256, 1, 3, 1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 33, 33)  # Reshape to (batch_size, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        # Reshape x to match LSTM input shape\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, flattened_height * flattened_width)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(x.size(0), -1, 16, 16)  # Reshape back to 2D spatial shape\n",
    "        x = F.relu(self.conv8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db603c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551eb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916b414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n",
      "Images shape on batch size = torch.Size([128, 272])\n",
      "Labels shape on batch size = torch.Size([128, 1089])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 1, 3, 3], but got 3-dimensional input of size [128, 1, 1089] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31784/3270802129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_state_new.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31784/3270802129.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31784/3270802129.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add a dummy channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 1, 3, 3], but got 3-dimensional input of size [128, 1, 1089] instead"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from TrainDataset import TrainDataset\n",
    "from TestDataset import TestDataset\n",
    "#from model import ReconNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pyplot\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "\n",
    "# This is the path of the BSDS200 dataset\n",
    "# The Berkeley Segmentation Dataset and Benchmark (BSDS 500 or BSDS 200) is a dataset\n",
    "# and evaluation benchmark widely used in the computer vision community for the task of image segmentation.\n",
    "path = './images/all_images/BSDS200'\n",
    "\n",
    "# We will use the phi_0_25_1089.mat file, the file has a compression ratio of 0.25 and has a dimension of 1089.\n",
    "# We will be using this because it has a higher compression ratio.\n",
    "mat = scipy.io.loadmat('./phi/phi_0_25_1089.mat')['phi']\n",
    "mat = torch.from_numpy(mat)\n",
    "\n",
    "# The train dataset images are compressed and divided into blocks and converted into pytorch tensor.\n",
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = TrainDataset(path, mat, transformations, 0.25)\n",
    "train_dl = DataLoader(train_data, batch_size=128)\n",
    "\n",
    "train_iter = iter(train_dl)\n",
    "print(type(train_iter))\n",
    "\n",
    "#images are the compressed images, and labels are the original images, compression ratio - 0.25 is used.\n",
    "#images , labels = train_iter.next()\n",
    "images , labels = next(train_iter)\n",
    "print('Images shape on batch size = {}'.format(images.size()))\n",
    "print('Labels shape on batch size = {}'.format(labels.size()))\n",
    "\n",
    "\n",
    "class ReconNet(nn.Module):\n",
    "    def __init__(self, measurement_rate=0.25):\n",
    "        super(ReconNet, self).__init__()\n",
    "        self.measurement_rate = measurement_rate\n",
    "        self.fc1 = nn.Linear(int(self.measurement_rate * 1089), 1089)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        self.lstm = nn.LSTM(input_size=256 * 16 * 16, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.conv8 = nn.Conv2d(256, 1, 3, 1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.unsqueeze(1)  # Add a dummy channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the spatial dimensions\n",
    "        x = x.unsqueeze(1)  # Add a dummy sequence dimension\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the LSTM output\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)  # Reshape to (batch_size, channels, height, width)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        return x\n",
    "\n",
    "model = ReconNet(measurement_rate=0.25)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, criterion, optimizer, train_dl, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for inp, lbl in train_dl:\n",
    "            inp = inp.cuda()\n",
    "            lbl = lbl.cuda()\n",
    "            inp = inp.float()\n",
    "            lbl = lbl.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(inp)\n",
    "            out = out.cuda()\n",
    "            out = out.view(lbl.size())\n",
    "            loss = criterion(out, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        print('Epoch:', epoch+1, 'Training Loss:', np.mean(train_loss))\n",
    "\n",
    "train(model, criterion, optimizer, train_dl, 10)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_state_new.pth')\n",
    "\n",
    "state_dict = torch.load('model_state_new.pth', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98f99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2aa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3ceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bb147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
